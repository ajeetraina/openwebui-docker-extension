# Docker Model Runner Configuration (llama.cpp engine)
BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1/
MODEL=ai/llama3.2:1B-Q8_0
API_KEY=${API_KEY:-dockermodelrunner}

# Alternative: Use Docker Compose model provider
OLLAMA_BASE_URL=http://llm:11434
LLM_MODEL_NAME=ai/llama3.2:1B-Q8_0

# OpenWebUI Configuration
WEBUI_NAME=OpenWebUI + Model Runner
WEBUI_AUTH=false
ENABLE_SIGNUP=true
ENABLE_OLLAMA_API=false
ENABLE_OPENAI_API=true

# OpenAI-compatible API settings (for Model Runner)
OPENAI_API_BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1
OPENAI_API_KEY=dockermodelrunner

# MCP Toolkit Integration
ENABLE_MCP=true
MCP_SERVERS=docker,github,filesystem

# Docker Integration
DOCKER_HOST=unix:///var/run/docker.sock

# Observability configuration
LOG_LEVEL=info
LOG_PRETTY=true
TRACING_ENABLED=true

# Additional Model Runner settings
MODEL_RUNNER_ENABLED=true
MODEL_RUNNER_ENDPOINT=http://model-runner.docker.internal
MODEL_RUNNER_ENGINE=llama.cpp

# Security
CORS_ALLOW_ORIGIN=*
ENABLE_MODEL_FILTER=true
ENABLE_COMMUNITY_SHARING=false
