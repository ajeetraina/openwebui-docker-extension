services:
  # OpenWebUI service configured for Docker Model Runner
  openwebui:
    build:
      context: .
      dockerfile: Dockerfile
    image: openwebui-model-runner:latest
    container_name: openwebui-model-runner
    env_file: './backend.env'
    volumes:
      - openwebui-data:/app/backend/data
      - /var/run/docker.sock:/var/run/docker.sock:ro
    ports:
      - "${OPENWEBUI_PORT-3001}:8080"
    environment:
      # Model Runner Configuration (Using llama.cpp engine)
      - 'OPENAI_API_BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1'
      - 'OPENAI_API_KEY=dockermodelrunner'
      
      # Alternative: Connect to Docker Compose model provider
      - 'OLLAMA_BASE_URL=http://llm:11434'
      
      # Disable Ollama API, use OpenAI-compatible instead
      - 'ENABLE_OLLAMA_API=false'
      - 'ENABLE_OPENAI_API=true'
      
      # MCP Toolkit Integration
      - 'ENABLE_MCP=true'
      - 'MCP_SERVERS=docker,github,filesystem'
      
      # WebUI Configuration
      - 'WEBUI_NAME=OpenWebUI + Model Runner'
      - 'WEBUI_SECRET_KEY='
      - 'WEBUI_AUTH=false'
      - 'ENABLE_SIGNUP=true'
      - 'DEFAULT_USER_ROLE=user'
      
      # Docker Integration
      - 'DOCKER_HOST=unix:///var/run/docker.sock'
      
      # Enhanced Features
      - 'ENABLE_MODEL_FILTER=true'
      - 'ENABLE_COMMUNITY_SHARING=false'
      - 'CORS_ALLOW_ORIGIN=*'
    
    extra_hosts:
      - "host.docker.internal:host-gateway"
      - "model-runner.docker.internal:host-gateway"
    
    depends_on:
      - llm
      - mcp-server
    
    networks:
      - openwebui-network
    
    restart: unless-stopped
    
    labels:
      - "com.docker.desktop.extension=true"
      - "com.docker.desktop.extension.api.version=>=0.3.4"
      - "openwebui.model-runner=true"

  # Docker Compose Model Provider (like in aiwatch)
  llm:
    provider:
      type: model
      options:
        model: ${LLM_MODEL_NAME:-ai/llama3.2:1B-Q8_0}
    networks:
      - openwebui-network

  # MCP Server for Docker/GitHub/Filesystem integration
  mcp-server:
    build:
      context: ./mcp
      dockerfile: Dockerfile
    container_name: openwebui-mcp-server
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - openwebui-data:/shared-data
      - mcp-data:/app/data
    environment:
      - 'MCP_SERVER_NAME=openwebui-mcp'
      - 'DOCKER_HOST=unix:///var/run/docker.sock'
      - 'MODEL_RUNNER_ENDPOINT=http://model-runner.docker.internal'
      # Connect to local model service
      - 'LLM_ENDPOINT=http://llm:11434'
    extra_hosts:
      - "host.docker.internal:host-gateway"
      - "model-runner.docker.internal:host-gateway"
    networks:
      - openwebui-network
    restart: unless-stopped
    labels:
      - "openwebui.mcp-server=true"

volumes:
  openwebui-data:
    driver: local
    labels:
      - "openwebui.data=true"
  mcp-data:
    driver: local
    labels:
      - "openwebui.mcp-data=true"

networks:
  openwebui-network:
    driver: bridge
    labels:
      - "openwebui.network=true"
