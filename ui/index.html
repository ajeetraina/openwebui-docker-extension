<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenWebUI + Model Runner</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        .docker-gradient { background: linear-gradient(135deg, #0db7ed 0%, #0ea5e9 100%); }
        .status-indicator { width: 8px; height: 8px; border-radius: 50%; display: inline-block; margin-right: 8px; }
        .status-connected { background-color: #10b981; }
        .status-connecting { background-color: #f59e0b; animation: pulse 2s infinite; }
        .status-disconnected { background-color: #ef4444; }
        @keyframes pulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.5; } }
        .endpoint-info { font-family: 'Courier New', monospace; font-size: 0.8em; background: #f1f5f9; padding: 4px 8px; border-radius: 4px; }
    </style>
</head>
<body class="bg-gray-50 min-h-screen">
    <div class="docker-gradient text-white p-6">
        <div class="max-w-7xl mx-auto flex items-center justify-between">
            <div class="flex items-center space-x-4">
                <svg class="w-10 h-10" viewBox="0 0 24 24" fill="currentColor">
                    <path d="M13.5 3.5L14 4H20C20.6 4 21 4.4 21 5V19C21 19.6 20.6 20 20 20H4C3.4 20 3 19.6 3 19V5C3 4.4 3.4 4 4 4H9.5L10 3.5C10.2 3.2 10.6 3 11 3H13C13.4 3 13.8 3.2 14 3.5Z"/>
                </svg>
                <div>
                    <h1 class="text-2xl font-bold">OpenWebUI + Model Runner</h1>
                    <p class="text-blue-100">AI Chat with Docker Model Runner (llama.cpp) & MCP</p>
                    <div class="endpoint-info text-blue-200 mt-1">
                        llama.cpp: http://model-runner.docker.internal/engines/llama.cpp/v1
                    </div>
                </div>
            </div>
            <div class="space-y-2">
                <div id="modelRunnerStatus" class="flex items-center bg-white/20 rounded-lg px-3 py-2">
                    <span class="status-indicator status-connecting"></span>
                    <span class="text-sm font-medium">Connecting to Model Runner...</span>
                </div>
                <div id="composeStatus" class="flex items-center bg-white/20 rounded-lg px-3 py-2">
                    <span class="status-indicator status-connecting"></span>
                    <span class="text-sm font-medium">Checking Compose LLM...</span>
                </div>
            </div>
        </div>
    </div>

    <div class="max-w-7xl mx-auto p-6">
        <div class="grid grid-cols-1 md:grid-cols-4 gap-4 mb-6">
            <div class="bg-white rounded-lg shadow p-6">
                <h3 class="font-semibold text-gray-900 mb-2 flex items-center">
                    <span class="w-3 h-3 bg-blue-500 rounded-full mr-2"></span>
                    Model Runner
                </h3>
                <div class="text-2xl font-bold text-gray-900" id="modelCount">0</div>
                <div class="text-sm text-gray-600">Models Available</div>
                <div class="text-xs text-gray-500 mt-1" id="modelEngine">llama.cpp engine</div>
            </div>
            <div class="bg-white rounded-lg shadow p-6">
                <h3 class="font-semibold text-gray-900 mb-2 flex items-center">
                    <span class="w-3 h-3 bg-purple-500 rounded-full mr-2"></span>
                    MCP Tools
                </h3>
                <div class="text-2xl font-bold text-gray-900" id="toolCount">7</div>
                <div class="text-sm text-gray-600">Tools Available</div>
                <div class="text-xs text-gray-500 mt-1">Docker + Model Runner</div>
            </div>
            <div class="bg-white rounded-lg shadow p-6">
                <h3 class="font-semibold text-gray-900 mb-2 flex items-center">
                    <span class="w-3 h-3 bg-green-500 rounded-full mr-2"></span>
                    Docker Compose
                </h3>
                <div class="text-2xl font-bold text-gray-900" id="composeModelCount">1</div>
                <div class="text-sm text-gray-600">Compose Models</div>
                <div class="text-xs text-gray-500 mt-1" id="composeModel">ai/llama3.2:1B-Q8_0</div>
            </div>
            <div class="bg-white rounded-lg shadow p-6">
                <h3 class="font-semibold text-gray-900 mb-2 flex items-center">
                    <span class="w-3 h-3 bg-red-500 rounded-full mr-2"></span>
                    System Health
                </h3>
                <div class="text-2xl font-bold text-green-600" id="healthStatus">Good</div>
                <div class="text-sm text-gray-600">Overall Status</div>
                <div class="text-xs text-gray-500 mt-1">aiwatch pattern</div>
            </div>
        </div>

        <div class="grid grid-cols-1 lg:grid-cols-2 gap-6 mb-6">
            <div class="bg-white rounded-lg shadow p-6">
                <h3 class="font-semibold text-gray-900 mb-4">Model Runner Endpoints</h3>
                <div class="space-y-2 text-sm">
                    <div class="flex justify-between">
                        <span class="text-gray-600">llama.cpp API:</span>
                        <code class="text-blue-600 bg-gray-100 px-2 py-1 rounded">/engines/llama.cpp/v1</code>
                    </div>
                    <div class="flex justify-between">
                        <span class="text-gray-600">Models List:</span>
                        <code class="text-blue-600 bg-gray-100 px-2 py-1 rounded">/v1/models</code>
                    </div>
                    <div class="flex justify-between">
                        <span class="text-gray-600">Chat Completions:</span>
                        <code class="text-blue-600 bg-gray-100 px-2 py-1 rounded">/chat/completions</code>
                    </div>
                    <div class="flex justify-between">
                        <span class="text-gray-600">Default Model:</span>
                        <code class="text-green-600 bg-gray-100 px-2 py-1 rounded">ai/llama3.2:1B-Q8_0</code>
                    </div>
                </div>
            </div>

            <div class="bg-white rounded-lg shadow p-6">
                <h3 class="font-semibold text-gray-900 mb-4">Quick Actions</h3>
                <div class="space-y-2">
                    <button onclick="testModelRunner()" class="w-full bg-blue-600 text-white py-2 px-4 rounded hover:bg-blue-700">
                        ðŸ§ª Test Model Runner
                    </button>
                    <button onclick="refreshStatus()" class="w-full bg-green-600 text-white py-2 px-4 rounded hover:bg-green-700">
                        ðŸ”„ Refresh Status
                    </button>
                    <button onclick="showEndpoints()" class="w-full bg-purple-600 text-white py-2 px-4 rounded hover:bg-purple-700">
                        ðŸ”— Show All Endpoints
                    </button>
                </div>
            </div>
        </div>

        <div class="bg-white rounded-lg shadow overflow-hidden">
            <div class="bg-gray-50 px-6 py-4 border-b">
                <h3 class="text-lg font-semibold text-gray-900">OpenWebUI Chat Interface</h3>
                <p class="text-sm text-gray-600">Powered by Docker Model Runner (llama.cpp engine)</p>
            </div>
            <iframe 
                src="http://localhost:3001"
                class="w-full border-0"
                style="height: 600px;"
                title="OpenWebUI Interface">
            </iframe>
        </div>
    </div>

    <script>
        const endpoints = {
            modelRunner: 'http://model-runner.docker.internal/v1/models',
            llamaCpp: 'http://model-runner.docker.internal/engines/llama.cpp/v1/models',
            composeLlm: 'http://llm:11434/api/tags'
        };

        async function checkModelRunner() {
            try {
                // Try llama.cpp endpoint first
                const response = await fetch(endpoints.llamaCpp);
                if (response.ok) {
                    const data = await response.json();
                    const count = data.data?.length || 0;
                    document.getElementById('modelCount').textContent = count;
                    document.getElementById('modelRunnerStatus').innerHTML = 
                        '<span class="status-indicator status-connected"></span><span class="text-sm font-medium">Model Runner Connected (' + count + ' models)</span>';
                    return true;
                }
            } catch (error) {
                console.log('llama.cpp endpoint not available:', error);
            }
            
            // Fallback to general Model Runner endpoint
            try {
                const response = await fetch(endpoints.modelRunner);
                if (response.ok) {
                    document.getElementById('modelRunnerStatus').innerHTML = 
                        '<span class="status-indicator status-connected"></span><span class="text-sm font-medium">Model Runner Connected (fallback)</span>';
                    return true;
                }
            } catch (error) {
                console.log('Model Runner not available:', error);
            }
            
            document.getElementById('modelRunnerStatus').innerHTML = 
                '<span class="status-indicator status-disconnected"></span><span class="text-sm font-medium">Model Runner Disconnected</span>';
            return false;
        }

        async function checkComposeLLM() {
            try {
                const response = await fetch(endpoints.composeLlm);
                if (response.ok) {
                    document.getElementById('composeStatus').innerHTML = 
                        '<span class="status-indicator status-connected"></span><span class="text-sm font-medium">Compose LLM Connected</span>';
                    return true;
                }
            } catch (error) {
                console.log('Compose LLM not available:', error);
            }
            
            document.getElementById('composeStatus').innerHTML = 
                '<span class="status-indicator status-disconnected"></span><span class="text-sm font-medium">Compose LLM Disconnected</span>';
            return false;
        }

        function testModelRunner() {
            alert('ðŸ§ª Testing Model Runner connectivity...\n\nChecking:\n- llama.cpp endpoint\n- Chat completions\n- Model availability\n\nSee browser console for details.');
            checkModelRunner();
            checkComposeLLM();
        }

        function refreshStatus() {
            checkModelRunner();
            checkComposeLLM();
        }

        function showEndpoints() {
            const info = `ðŸ”— Model Runner Endpoints:

llama.cpp API:
${endpoints.llamaCpp}

Model Runner API:
${endpoints.modelRunner}

Docker Compose LLM:
${endpoints.composeLlm}

Chat Completions:
POST http://model-runner.docker.internal/engines/llama.cpp/v1/chat/completions

Headers:
Authorization: Bearer dockermodelrunner
Content-Type: application/json

Default Model: ai/llama3.2:1B-Q8_0`;

            alert(info);
        }
        
        // Initialize
        document.addEventListener('DOMContentLoaded', () => {
            checkModelRunner();
            checkComposeLLM();
            setInterval(() => {
                checkModelRunner();
                checkComposeLLM();
            }, 30000);
        });
    </script>
</body>
</html>
